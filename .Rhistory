bag.fraction = 0.8,
train.fraction = 1.0,
cv.folds=0,
keep.data = TRUE,
verbose = "CV",
class.stratify.cv=NULL,
n.cores = NULL)
})
rm(list = ls())
library(EBImage)
library(ranger)
library(MyHelperFunctions)
library(caret)
library(gtools)
library(gbm)
library(pryr)
plotMNIST <- defmacro(x, expr = {
Im1 <- matrix(as.numeric(MNISTtrain[x, -1]), 28, 28, byrow = T)
Im2 <- t(apply(Im1, 2, rev)) # the image function for some reason draws bottom up
graphics::image(z = Im2, col = grey(seq(1, 0, length = 256)))
})
#---------------------------------------------------------------------------------------------
MNISTtrain <- read.csv("D:/29 ImageRecognition/R/Data/MNIST/mnist_train.csv", header = F)
colnames(MNISTtrain) <- c("Y", paste0("P", 1:784))
MNISTtrain$Y <- as.factor(MNISTtrain$Y)
#---------------------------------------------------------------------------------------------
Xs <- paste0("P", 1:784)
Y <- "Y"
set.seed(432)
sampVec <- sample(1:60000, 20000)
Train <- MNISTtrain[1:50000,]
Val <- MNISTtrain[50001:60000,]
mem_used()
object_size(gbm1)
system.time({
set.seed(12345)
gbm1 <- gbm(formula = create.formula(Y, Xs),
distribution = "multinomial",
data = Train[1:100],
# weights,
# var.monotone = NULL,
n.trees = 2,
interaction.depth = 1,
n.minobsinnode = 10,
shrinkage = 0.001,
bag.fraction = 0.8,
train.fraction = 1.0,
cv.folds=0,
keep.data = TRUE,
verbose = "CV",
class.stratify.cv=NULL,
n.cores = NULL)
})
system.time({
set.seed(12345)
gbm1 <- gbm(formula = create.formula(Y, Xs),
distribution = "multinomial",
data = Train[1:100,],
# weights,
# var.monotone = NULL,
n.trees = 2,
interaction.depth = 1,
n.minobsinnode = 10,
shrinkage = 0.001,
bag.fraction = 0.8,
train.fraction = 1.0,
cv.folds=0,
keep.data = TRUE,
verbose = "CV",
class.stratify.cv=NULL,
n.cores = NULL)
})
warnings()
object_size(gbm1)
mem_used()
system.time({
set.seed(12345)
gbm1 <- gbm(formula = create.formula(Y, Xs),
distribution = "multinomial",
data = Train[1:100,],
# weights,
# var.monotone = NULL,
n.trees = 2,
interaction.depth = 1,
n.minobsinnode = 10,
shrinkage = 0.001,
bag.fraction = 0.8,
train.fraction = 1.0,
cv.folds=0,
keep.data = TRUE,
verbose = "CV",
class.stratify.cv=NULL,
n.cores = NULL)
})
system.time({
set.seed(12345)
gbm1 <- gbm(formula = create.formula(Y, Xs),
distribution = "multinomial",
data = Train[1:100,],
# weights,
# var.monotone = NULL,
n.trees = 100,
interaction.depth = 1,
n.minobsinnode = 10,
shrinkage = 0.001,
bag.fraction = 0.8,
train.fraction = 1.0,
cv.folds=0,
keep.data = TRUE,
verbose = "CV",
class.stratify.cv=NULL,
n.cores = NULL)
})
system.time({
set.seed(12345)
gbm1 <- gbm(formula = create.formula(Y, Xs),
distribution = "multinomial",
data = Train[1:1000,],
# weights,
# var.monotone = NULL,
n.trees = 100,
interaction.depth = 1,
n.minobsinnode = 10,
shrinkage = 0.001,
bag.fraction = 0.8,
train.fraction = 1.0,
cv.folds=0,
keep.data = TRUE,
verbose = "CV",
class.stratify.cv=NULL,
n.cores = NULL)
})
pred <- predict(gbm1, data = Val)$predictions
pred <- predict(gbm1, data = Val)$predictions
class(gbm1)
?predict.gbm
pred <- predict(gbm1, newdata = Val, n.trees = 100)$predictions
rm(list = ls())
library(EBImage)
library(ranger)
library(MyHelperFunctions)
library(caret)
library(gtools)
library(gbm)
library(pryr)
plotMNIST <- defmacro(x, expr = {
Im1 <- matrix(as.numeric(MNISTtrain[x, -1]), 28, 28, byrow = T)
Im2 <- t(apply(Im1, 2, rev)) # the image function for some reason draws bottom up
graphics::image(z = Im2, col = grey(seq(1, 0, length = 256)))
})
#---------------------------------------------------------------------------------------------
MNISTtrain <- read.csv("D:/29 ImageRecognition/R/Data/MNIST/mnist_train.csv", header = F)
colnames(MNISTtrain) <- c("Y", paste0("P", 1:784))
MNISTtrain$Y <- as.factor(MNISTtrain$Y)
#---------------------------------------------------------------------------------------------
Xs <- paste0("P", 1:784)
Y <- "Y"
set.seed(432)
sampVec <- sample(1:60000, 20000)
Train <- MNISTtrain[1:50000,]
Val <- MNISTtrain[50001:60000,]
system.time({
set.seed(12345)
gbm1 <- gbm(formula = create.formula(Y, Xs),
distribution = "multinomial",
data = Train[1:1000,],
# weights,
# var.monotone = NULL,
n.trees = 100,
interaction.depth = 1,
n.minobsinnode = 10,
shrinkage = 0.001,
bag.fraction = 0.8,
train.fraction = 1.0,
cv.folds=0,
keep.data = TRUE,
verbose = "CV",
class.stratify.cv=NULL,
n.cores = NULL)
})
pred <- predict(gbm1, newdata = Val, n.trees = 100)$predictions
pred <- predict(gbm1, newdata = Val, n.trees = 100)
pred
dim(pred)
100*10
class(pred)
pred[1,1,1]
pred[2,1,1]
pred[10000,1,1]
pred[10000,1,]
pred[10000,1]
pred[1:10000,1]
pred[1:10000,1,]
pred[1:10000,1,1]
pred[1:10000,1,2]
pred[1:10000,1,1]
View(pred[1:10000,1,1])
pred <- predict(gbm1, newdata = Val, n.trees = 100, type="response")
pred
pred
dim(pred)
10*1000
View(pred)
pred <- predict(gbm1, newdata = Val, n.trees = 1, type="response")
pred
View(pred)
pred <- predict(gbm1, newdata = Val, n.trees = 1000, type="response")
pred <- predict(gbm1, newdata = Val, n.trees = 100, type="response")
pred <- predict(gbm1, newdata = Train[1:1000,], n.trees = 100, type="response")
pred
View(pred)
?which
?max.col
max.col(pred)
max.col(pred[,,1])
pred <- max.col(pred[,,1])
obs <- Val$Y
confusionMatrix(obs, pred)
pred
pred <- max.col(pred[,,1]) - 1
View(pred[,,1])
pred <- pred - 1
pred
obs <- Val$Y
confusionMatrix(obs, pred)
pred <- predict(gbm1, newdata = Val, n.trees = 100, type="response")
pred <- max.col(pred[,,1])
pred <- pred - 1
obs <- Val$Y
confusionMatrix(obs, pred)
Wrong <- data.frame(pred,obs)
Wrong <- Wrong[pred != obs,]
View(Wrong)
View(Wrong)
x <- 1
plotMNIST(x)
View(Wrong)
View(Wrong)
View(Wrong)
plotMNIST(2)
View(Wrong)
Wrong <- data.frame(pred,obs)
View(Wrong)
View(Val)
Wrong <- data.frame(pred,obs)
Wrong <- Wrong[pred != obs,]
View(Wrong)
x <- 50000 + 1
plotMNIST(x)
View(Wrong)
system.time({
set.seed(12345)
gbm1 <- gbm(formula = create.formula(Y, Xs),
distribution = "multinomial",
data = Train[1:10000,],
# weights,
# var.monotone = NULL,
n.trees = 100,
interaction.depth = 1,
n.minobsinnode = 10,
shrinkage = 0.001,
bag.fraction = 0.8,
train.fraction = 1.0,
cv.folds=0,
keep.data = TRUE,
verbose = "CV",
class.stratify.cv=NULL,
n.cores = NULL)
})
object_size(gbm1)
mem_used()
object_size(MNISTtrain)
attributes(gbm1)
pred <- predict(gbm1, newdata = Val, n.trees = 100, type="response")
pred <- max.col(pred[,,1])
pred <- pred - 1
obs <- Val$Y
confusionMatrix(obs, pred)
pred <- predict(gbm1, newdata = Train[1:10000,], n.trees = 100, type="response")
pred <- max.col(pred[,,1])
pred <- pred - 1
#------------------------------------------------------------------------------------------------------------
obs <- Val$Y
# pred <- predict(ranger1, data = Val)$predictions
confusionMatrix(obs, pred)
Wrong <- data.frame(pred,obs)
Wrong <- Wrong[pred != obs,]
Train <- MNISTtrain[1:10000,]
system.time({
set.seed(12345)
gbm1 <- gbm(formula = create.formula(Y, Xs),
distribution = "multinomial",
data = Train,
# weights,
# var.monotone = NULL,
n.trees = 200,
interaction.depth = 1,
n.minobsinnode = 10,
shrinkage = 0.001,
bag.fraction = 0.8,
train.fraction = 1.0,
cv.folds=0,
keep.data = FALSE,
verbose = "CV",
class.stratify.cv=NULL,
n.cores = NULL)
})
object_size(gbm1)
mem_used()
pred <- predict(gbm1, newdata = Train, n.trees = 100, type="response")
pred <- max.col(pred[,,1])
pred <- pred - 1
mem_used()
object_size(gbm1)
pred <- predict(gbm1, newdata = Train, n.trees = 100, type="response")
pred <- max.col(pred[,,1])
pred <- pred - 1
obs <- Val$Y
obs <- Train$Y
confusionMatrix(obs, pred)
system.time({
set.seed(12345)
gbm1 <- gbm(formula = create.formula(Y, Xs),
distribution = "multinomial",
data = Train,
# weights,
# var.monotone = NULL,
n.trees = 200,
interaction.depth = 2,
n.minobsinnode = 10,
shrinkage = 0.001,
bag.fraction = 0.8,
train.fraction = 1.0,
cv.folds=0,
keep.data = FALSE,
verbose = "CV",
class.stratify.cv=NULL,
n.cores = NULL)
})
9^5
5^9
5^4
5^9
pred <- predict(gbm1, newdata = Train, n.trees = 100, type="response")
pred <- max.col(pred[,,1])
pred <- pred - 1
obs <- Train$Y
confusionMatrix(obs, pred)
500/60
library(tensorflow)
rm(list = ls())
library(tensorflow)
x_data <- runif(100, min=0, max=1)
y_data <- x_data * 0.1 + 0.3
W <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))
b <- tf$Variable(tf$zeros(shape(1L)))
y <- W * x_data + b
loss <- tf$reduce_mean((y - y_data) ^ 2)
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
sess = tf$Session()
sess$run(tf$global_variables_initializer())
for (step in 1:201) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
W
y
sess$run(W)
sess$run(b)
class(optimizer)
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
class(tf$nn)
class(tf$Session)
class(tf$constant())
class(tf$constant
)
class(1L)
class(1)
?dict
?reticulate
dict(x = "asd", dragan = 1:10)
?with
?%as%
rm(list = ls())
library(EBImage)
library(ranger)
library(MyHelperFunctions)
library(caret)
library(gtools)
library(gbm)
library(pryr)
library(tensorflow)
plotMNIST <- defmacro(x, expr = {
Im1 <- matrix(as.numeric(MNISTtrain[x, -1]), 28, 28, byrow = T)
Im2 <- t(apply(Im1, 2, rev)) # the image function for some reason draws bottom up
graphics::image(z = Im2, col = grey(seq(1, 0, length = 256)))
})
MNISTtrain <- read.csv("D:/29 ImageRecognition/R/Data/MNIST/mnist_train.csv", header = F)
colnames(MNISTtrain) <- c("Y", paste0("P", 1:784))
MNISTtrain$Y <- as.factor(MNISTtrain$Y)
#---------------------------------------------------------------------------------------------
Xs <- paste0("P", 1:784)
Y <- "Y"
set.seed(432)
sampVec <- sample(1:60000, 20000)
Train <- MNISTtrain[1:10000,]
Val <- MNISTtrain[50001:60000,]
x <- 50000 + 1
plotMNIST(x)
?graphics::image
plotMNIST <- defmacro(x, expr = {
Im1 <- matrix(as.numeric(MNISTtrain[x, -1]), 28, 28, byrow = T)
Im2 <- t(apply(Im1, 2, rev)) # the image function for some reason draws bottom up
graphics::image(z = Im2, col = grey(seq(0, 1, length = 256)))
})
plotMNIST(x)
plotMNIST <- defmacro(x, expr = {
Im1 <- matrix(as.numeric(MNISTtrain[x, -1]), 28, 28, byrow = T)
Im2 <- t(apply(Im1, 2, rev)) # the image function for some reason draws bottom up
graphics::image(z = Im2, col = grey(seq(1, 0, length = 256)))
})
plotMNIST(x)
View(Im1)
rm(list = ls())
library(EBImage)
library(ranger)
library(MyHelperFunctions)
library(caret)
library(gtools)
library(gbm)
library(pryr)
library(tensorflow)
?tf$constant
?tf$constant
rm(list = ls())
library(EBImage)
library(ranger)
library(MyHelperFunctions)
library(caret)
library(gtools)
library(gbm)
library(pryr)
library(tensorflow)
plotMNIST <- defmacro(x, expr = {
Im1 <- matrix(as.numeric(MNISTtrain[x, -1]), 28, 28, byrow = T)
Im2 <- t(apply(Im1, 2, rev)) # the image function for some reason draws bottom up
graphics::image(z = Im2, col = grey(seq(1, 0, length = 256)))
})
MNISTtrain <- read.csv("D:/29 ImageRecognition/R/Data/MNIST/mnist_train.csv", header = F)
colnames(MNISTtrain) <- c("Y", paste0("P", 1:784))
MNISTtrain$Y <- as.factor(MNISTtrain$Y)
Xs <- paste0("P", 1:784)
Y <- "Y"
set.seed(432)
sampVec <- sample(1:60000, 20000)
Train <- MNISTtrain[1:10000,]
Val <- MNISTtrain[50001:60000,]
system.time({
set.seed(12345)
ranger1 <- ranger(create.formula(Y, Xs),
data = Train,
num.trees = 1000)
})
View(MNISTtrain)
?devtools::install_github()
library(devtools)
?install_github
devtools::install_github("rstudio/tensorflow")
devtools::install_github("rstudio/tensorflow")
devtools::install_github("rstudio/tensorflow")
library(installr)
updateR()
devtools::install_github("rstudio/tensorflow")
devtools::install_github("rstudio/tensorflow")
devtools::install_github("rstudio/tensorflow")
install.packages("Rcpp")
Rcpp::evalCpp("2+2")
cat('Sys.setenv(BINPREF = "C:/RBuildTools/3.4/mingw_64/bin',
file = file.path(Sys.getenv("HOME"), ".Rprofile"),
sep = "\n", append = TRUE)
devtools::install_github("rstudio/tensorflow")
library(tensorflow)
datasets <- tf$contrib$learn$datasets
plo
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
View(mnist)
View(datasets)
View(mnist)
rm(list = ls())
library(EBImage)
library(ranger)
library(MyHelperFunctions)
library(caret)
library(gtools)
library(gbm)
library(pryr)
library(tensorflow)
plotMNIST <- defmacro(x, expr = {
Im1 <- matrix(as.numeric(MNISTtrain[x, -1]), 28, 28, byrow = T)
Im2 <- t(apply(Im1, 2, rev)) # the image function for some reason draws bottom up
graphics::image(z = Im2, col = grey(seq(1, 0, length = 256)))
})
mnist$train
library(tensorflow)
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
mnist$train
mnist$train$next_batch
rm(list = ls())
